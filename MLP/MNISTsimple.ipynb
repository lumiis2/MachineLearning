{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importação de bibliotecas importantes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição da nossa rede de Multilayer Perceptron (MLP)\n",
    "Nossa rede herda de nn.Module e é construída com especificações para o tamanho das camadas de entrada, camada oculta e camada de saída.\n",
    "Para o dataset MNIST, o tamanho da camada de saída é 10, pois estamos classificando dígitos de 0 a 9.\n",
    "A classe faz uma estrutura genérica em sua inicialização e define as funções que serão utilizadas.\n",
    "Depois, definimos que para cada camada faremos o uso de nn.Linear, que representa a aplicação da fórmula y = W^T.x + b.\n",
    "Após isso, definimos como será feito o nosso forward pass: receberemos uma matriz de valores de entrada e faremos um flatten\n",
    "nela (empilhando suas linhas para formar um vetor), o que gerará nosso vetor de entrada. Em seguida, para cada camada, aplicaremos uma função\n",
    "de ativação chamada ReLU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # Função de ativação ReLU e camadas lineares (fully connected)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "        self.hidden_layer = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Achatamos os dados de entrada e aplicamos a função ReLU em camadas sucessivas\n",
    "        flattened_x = self.flatten(x)\n",
    "        out = self.relu(self.input_layer(flattened_x))\n",
    "        out = self.relu(self.hidden_layer(out))\n",
    "        return self.output_layer(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuração do Modelo, Função de Custo e Otimizador\n",
    "\n",
    "Aqui instanciamos nosso modelo NeuralNetwork, definimos a função de custo (CrossEntropyLoss) e o otimizador (Adam).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(input_size=28*28, hidden_size=64, output_size=10)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregamento dos Dados\n",
    "\n",
    "Carregamos os datasets MNIST, dividimos em conjuntos de treinamento e teste e transformamos em tensores. \n",
    "Em seguida, preparamos os dados com DataLoader para uso na rede.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST('./', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST('./', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuração de Hiperparâmetros\n",
    "\n",
    "Aqui definimos o número de épocas (quantas vezes o modelo passará sobre o conjunto de treinamento) e definimos o total de minibatches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "total_batch = len(train_dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento do Modelo\n",
    "\n",
    "Aqui realizamos o treinamento do modelo, iterando por cada época, calculando gradientes e atualizando pesos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, step: 300/938: loss: 0.25446248054504395\n",
      "epoch 0, step: 600/938: loss: 0.31614920496940613\n",
      "epoch 0, step: 900/938: loss: 0.16125257313251495\n",
      "epoch 1, step: 300/938: loss: 0.09956208616495132\n",
      "epoch 1, step: 600/938: loss: 0.07718589901924133\n",
      "epoch 1, step: 900/938: loss: 0.12169670313596725\n",
      "epoch 2, step: 300/938: loss: 0.11994961649179459\n",
      "epoch 2, step: 600/938: loss: 0.01796025224030018\n",
      "epoch 2, step: 900/938: loss: 0.12944450974464417\n",
      "epoch 3, step: 300/938: loss: 0.11952754855155945\n",
      "epoch 3, step: 600/938: loss: 0.06157275289297104\n",
      "epoch 3, step: 900/938: loss: 0.02224172092974186\n",
      "epoch 4, step: 300/938: loss: 0.14250144362449646\n",
      "epoch 4, step: 600/938: loss: 0.004898659884929657\n",
      "epoch 4, step: 900/938: loss: 0.12872643768787384\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    batch_count = 0\n",
    "\n",
    "    for X, y in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_count += 1\n",
    "\n",
    "        # A cada 300 minibatches, a perda é impressa para monitorar o progresso.\n",
    "        if batch_count % 300 == 0:\n",
    "            print(f'epoch {epoch}, step: {batch_count}/{total_batch}: loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste do Modelo\n",
    "\n",
    "Aqui fazemos o teste do modelo após o treinamento e calculamos a acurácia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean train accuracy: 0.98146\n",
      "Mean test accuracy: 0.97134\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "\n",
    "    for X, y in train_dataloader:\n",
    "        predictions = model(X)\n",
    "        pred_values, pred_indexes = torch.max(predictions, dim=-1)\n",
    "\n",
    "        # Cálculo da acurácia comparando as previsões com os rótulos reais.\n",
    "        acc = sum(pred_indexes == y) / len(y)\n",
    "        train_acc.append(acc)\n",
    "\n",
    "    for X, y in test_dataloader:\n",
    "        predictions = model(X)\n",
    "        pred_values, pred_indexes = torch.max(predictions, dim=-1)\n",
    "\n",
    "        # Cálculo da acurácia comparando as previsões com os rótulos reais.\n",
    "        acc = sum(pred_indexes == y) / len(y)\n",
    "        test_acc.append(acc)\n",
    "\n",
    "    print(f'Mean train accuracy: {np.mean(train_acc):.5f}')\n",
    "    print(f'Mean test accuracy: {np.mean(test_acc):.5f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
